\section{Developing an Approximation}
\label{sec:method-of-moments}

Now that we have gotten to know our new distribution a little better, we can
use it to develop an approximation for the binomial.

Let $B \sim Bin(n, p)$ and $Y \sim SN(\mu, \sigma, \lambda)$. We will find
estimates for $\mu$, $\sigma$, and $\lambda$ using the method of moments, that
is, by comparing the first, second, and third moments about the mean (also
referred to as central moments) of $B$ and $Y$.

\subsection{The Moments of the Binomial Distribution}

Let's start with the binomial. The first two central moments are simply the
mean and variance:

\begin{equation*}
  E(B) = np, \quad Var(B) = np(1-p).
\end{equation*}

From these, we can find $E(B^2)$: Recall that $Var(B) = E(B^2) - [E(B)]^2$ and
thus

\begin{equation*}
  E(B^2) = Var(B) + [E(B)]^2 = np(1-p) + n^2p^2 = np - np^2 + n^2p^2.
\end{equation*}

We will also need $E(B^3)$, which we will get via the third factorial moment:

\begin{align}
  E[B(B-1)(B-2)] &= \sum_{x=0}^n x (x-1) (x-2) \cdot \left\{ \binom{n}{x} p^x q^{n-x} \right\}. \nonumber \\
  \intertext{Notice that the first three terms of the sum on the right hand side are zero, so we can rewrite it beginning at $x = 3$:}
  E[B(B-1)(B-2)] &= \sum_{x=3}^n x (x-1) (x-2) \cdot \left\{ \binom{n}{x} p^x q^{n-x} \right\}. \nonumber \\
  &= \sum_{x=3}^n x(x-1)(x-2) \cdot \frac{n!}{x!\;(n-x)!} \; p^x q^{n-x} \nonumber \\
  &= \sum_{x=3}^n \frac{n!}{(x-3)!\;(n-x)!} \; p^x q^{n-x} \nonumber \\
  &= \sum_{x=3}^n n(n-1)(n-2) p^3 \cdot \frac{(n-3)!}{(x-3)!\;(n-x)!} \; p^{x-3}q^{n-x} \;; \nonumber \\
  \intertext{let $y=x-3$; then $x=y+3$, and $x=3, x=n \Ra y=0, y=n-3$:}
  &= n(n-1)(n-2)p^3 \cdot \sum_{y=0}^{n-3} \frac{(n-3)!}{y!\;(n-(y+3))!} \; p^y q^{n-(y+3)} \nonumber \\
  &= n(n-1)(n-2)p^3 \cdot \underbrace {\sum_{y=0}^{n-3} \frac{(n-3)!}{y!\;((n-3)-y)!} \; p^y q^{(n-3)-y}}_{\mathclap{\textnormal{[pdf of $Bin(n-3,p)$ summed over its domain] = 1}}} \nonumber \\
  &= n(n-1)(n-2)p^3 \nonumber \\
  &= n^3p^3 - 3n^2p^3 + 2np^3. \label{eq:binomial-third-moment-rhs}
\end{align}

Here we take a moment to expand the left side of the previous equation:

\begin{align}
  E[B(B-1)(B-2)] &= E \left[ B^3 - 3B^2 + 2B \right] \nonumber \\
  &= E(B^3) - 3E(B^2) + 2E(B) \nonumber \\
  &= E(B^3) - 3(np - np^2 + n^2p^2) + 2np \nonumber \\
  &= E(B^3) - 3np + 3np^2 - 3n^2p^2 + 2np \nonumber \\
  &= E(B^3) + 3np^2 - 3n^2p^2 - np. \label{eq:binomial-third-moment-lhs}
\end{align}

We now set the left hand side \eqref{eq:binomial-third-moment-lhs} and right
hand side \eqref{eq:binomial-third-moment-rhs} equal to each other and solve
for $E(B^3)$:

\begin{align}
  E(B^3) + 3np^2 - 3n^2p^2 - np &= n^3p^3 - 3n^2p^3 + 2np^3 \nonumber \\
  \Rightarrow \qquad E(B^3) &= n^3p^3 - 3n^2p^3 + 2np^3 - 3np^2 + 3n^2p^2 + np. \label{eq:binomial-third-moment}
\end{align}

Now, finally, we have all the information necessary to obtain the third central
moment:

\begin{align*}
  E \left( [B - E(B)]^3 \right) &= E \left( B^3 - 3B^2 E(B) + 3B [E(B)]^2 - [E(B)]^3 \right) \\
  &= E(B^3) - 3 E(B^2) E(B) + 3 E(B) [E(B)]^2 - [E(B)]^3 \\
  &= E(B^3) - 3 E(B^2) E(B) + 2 [E(B)]^3 \\
  &= (n^3p^3 - 3n^2p^3 + 2np^3 - 3np^2 + 3n^2p^2 + np) - 3(np - np^2 + n^2p^2)(np) + 2(np)^3 \\
  &= \cancel{n^3p^3} - \cancel{3n^2p^3} + 2np^3 - 3np^2 + \cancel{3n^2p^2} + np - \cancel{3n^2p^2} + \cancel{3n^2p^3} - \cancel{3n^3p^3} + \cancel{2n^3p^3} \\
  &= 2np^3 - 3np^2 + np \\
  &= np(p-1)(2p-1).
\end{align*}

Our hard-earned results, restated for convenience:

\begin{align}
  E(B) &= np, \nonumber \\
  E([B - E(B)]^2) &= np(1-p), \\
  E([B - E(B)]^3) &= np(p-1)(2p-1). \nonumber
\end{align}

\subsection{The Moments of the Skew Normal Distribution}

Now we'll take a look at the skew normal. Equations
\eqref{eq:sn-moments-about-origin} and \eqref{eq:sn-variance} takes care of the
mean and variance; again the third central moment is a little more complicated:

\begin{align*}
  E([Y - E(Y)]^3) &= E(Y^3) - 3E(Y^2)E(Y) + 2[E(Y)]^3 \\
  &= (\mu^3 + 3 b \delta \mu^2 \sigma + 3 \mu \sigma^2 + 3 b \delta \sigma^3 - b \delta^3 \sigma^3) - 3 (\mu^2 + 2b \delta \mu \sigma + \sigma^2) (\mu + b \delta \sigma) \\
  & \quad + 2\;(\mu + b \delta \sigma)^3 \\
  &= \cancel{\mu^3} + \cancel{3 b \delta \mu^2 \sigma} + \cancel{3 \mu \sigma^2} + \cancel{3 b \delta \sigma^3} - b \delta^3 \sigma^3 - \cancel{3 \mu^3} - \cancel{3 b \delta \mu^2 \sigma} -
    \cancel{6 b \delta \mu^2 \sigma} - \cancel{6 b^2 \delta^2 \mu \sigma^2} - \cancel{3 \mu \sigma^2} \\
  & \quad - \cancel{3 b \delta \sigma^3} + \cancel{2 \mu^3} + \cancel{6 b \delta \mu^2 \sigma} + \cancel{6 b^2 \delta^2 \mu \sigma^2} + 2 b^3 \delta^3 \sigma^3 \\
  &= 2 b^3 \delta^3 \sigma^3 - b \delta^3 \sigma^3 \\
  &= b \delta^3 \sigma^3 (2b^2 - 1).
\end{align*}

We restate our results:

\begin{alignat}{4}
  E(Y) &= \mu + b \delta \sigma \;&=&\; \mu + \sigma \cdot \sqrt{\frac{2}{\pi}} \cdot \frac{\lambda}{\sqrt{1 + \lambda^2}} \;, \nonumber \\
  E([Y - E(Y)]^2) &= \sigma^2 (1 - b^2 \delta^2) \;&=&\; \sigma^2 \left( 1 - \frac{2}{\pi} \cdot \frac{\lambda^2}{1 + \lambda^2} \right), \\
  E([Y - E(Y)]^3) &= b \delta^3 \sigma^3 (2b^2 - 1) \;&=&\; \sigma^3 \sqrt{\frac{2}{\pi}} \left( \frac{\lambda}{\sqrt{1 + \lambda^2}} \right)^3 \left( \frac{4}{\pi} - 1 \right). \nonumber
\end{alignat}

\subsection[Solving for mu, sigma, lambda]{Solving for $\mu$, $\sigma$, $\lambda$}
\label{subsec:solving-for-mu-sigma-lambda}

To derive our approximation, we set the above moments of our two distributions
equal to each other and, taking $n$ and $p$ as constants, solve for $\mu$,
$\sigma$ and $\lambda$:

\begin{subequations}
\begin{align}
  np &= \mu + \sigma \cdot \sqrt{\frac{2}{\pi}} \cdot \frac{\lambda}{\sqrt{1 + \lambda^2}} \label{eq:first-moment-set} \\
  np(1-p) &= \sigma^2 \left( 1 - \frac{2}{\pi} \cdot \frac{\lambda^2}{1 + \lambda^2} \right) \label{eq:second-moment-set} \\
  np(p-1)(2p-1) &= \sigma^3 \sqrt{\frac{2}{\pi}} \left( \frac{\lambda}{\sqrt{1 + \lambda^2}} \right)^3 \left( \frac{4}{\pi} - 1 \right) \label{eq:third-moment-set}
\end{align}
\end{subequations}

To get $\lambda$, we divide the cube of \eqref{eq:second-moment-set} by the
square of \eqref{eq:third-moment-set}:

\begin{align}
  \frac{\sigma^6 \left( 1 - \frac{2}{\pi} \cdot \frac{\lambda^2}{1 + \lambda^2} \right)^3}{\sigma^6 \cdot \frac{2}{\pi} \left( \frac{\lambda}{\sqrt{1 + \lambda^2}} \right)^6 \left(
    \frac{4}{\pi} - 1 \right)^2} &= \frac{n^3p^3(1-p)^3}{n^2p^2(p-1)^2(2p-1)^2} \nonumber \\
  \Rightarrow \quad \frac{\left( 1 - \frac{2}{\pi} \cdot \frac{\lambda^2}{1+\lambda^2} \right)^3}{\frac{2}{\pi} \left( \frac{\lambda^2}{1+\lambda^2} \right)^3 \left( \frac{4}{\pi} - 1
    \right)^2} &= \frac{np(1-p)}{(1-2p)^2} \;. \label{eq:solving-for-lambda}
\end{align}

The above equation \eqref{eq:solving-for-lambda} is a rational expression in
$\lambda^2$ that can be solved with either a considerable amount of manual
labor or, more efficiently, with a computer algebra system. Once we have
$\lambda^2$, then $\lambda$ is simply either the positive or negative square
root, as determined by the sign of $(1-2p)$. The sign can be explained with a
little assistance from Property \ref{prop:3}: When $p \to 0$, the binomial
skews left and converges toward the positive half normal, which by
\eqref{eq:p2-positive-half-normal} corresponds to a positive $\lambda$. When $p
\to 1$, the binomial skews right and converges toward the negative half normal,
which by \eqref{eq:p2-negative-half-normal} corresponds to a negative
$\lambda$. When $p = 0.5$, the binomial is symmetric and $\lambda$ is 0,
eliminating the need for a sign. Thus:

\begin{equation}
  \label{eq:lambda-solved}
  \lambda = \textnormal{\{sign of $(1-2p)$\}} \sqrt{\lambda^2}.
\end{equation}

Having secured $\lambda$, we can find $\sigma$ using
\eqref{eq:second-moment-set}:

\begin{equation}
  \label{eq:sigma-solved}
  np(1-p) = \sigma^2 \left( 1 - \frac{2}{\pi} \cdot \frac{\lambda^2}{1 + \lambda^2} \right) \quad\Rightarrow\quad
  \sigma = \sqrt{\frac{np(1-p)}{1 - \frac{2}{\pi} \cdot \frac{\lambda^2}{1 + \lambda^2}}} \;.
\end{equation}

And with both $\lambda$ and $\sigma$, a simple rearrangement of
\eqref{eq:first-moment-set} yields $\mu$:

\begin{equation}
  \label{eq:mu-solved}
  np = \mu + \sigma \cdot \sqrt{\frac{2}{\pi}} \cdot \frac{\lambda}{\sqrt{1 + \lambda^2}} \quad\Rightarrow\quad
  \mu = np - \sigma \cdot \sqrt{\frac{2}{\pi}} \cdot \frac{\lambda}{\sqrt{1 + \lambda^2}} \;.
\end{equation}

One notable case where the above skew-normal approximation fails is when $p =
0.5$; the right side of \eqref{eq:solving-for-lambda} becomes undefined, and we
are unable to obtain $\lambda$. Fortunately, here we can fall back on our
intuition, which tells us that since the binomial is perfectly symmetrical, the
skew should be 0. A few observations support this conclusion: When $p = 0.5$,
equation \eqref{eq:lambda-solved} fails to yield a sign. Furthermore, when
$\lambda = 0$, equations \eqref{eq:sigma-solved} and \eqref{eq:mu-solved}
return us to the normal approximation ($\mu = np$ and $\sigma =
\sqrt{np(1-p)}$, respectively), which is after all a natural choice for a
symmetric binomial curve.

\subsection{Restrictions}

Unfortunately, although better than the normal approximation, the skew-normal
is also not universally applicable. To obtain an estimate for $\lambda$, we
must put a few restrictions on $n$ and $p$.

If we let $u = \frac{\lambda^2}{1+\lambda^2}$ and $v = 1/u$, we can rewrite the
left hand side of \eqref{eq:solving-for-lambda} as

\begin{alignat}{3}
  \frac{\left( 1 - \frac{2}{\pi} \cdot \frac{\lambda^2}{1+\lambda^2} \right)^3}{\frac{2}{\pi} \left( \frac{\lambda^2}{1+\lambda^2} \right)^3 \left( \frac{4}{\pi} - 1 \right)^2}
    &= \left. \left( 1 - \frac{2}{\pi} u \right)^3 \middle/ \frac{2}{\pi} u^3 \left( \frac{4}{\pi} - 1 \right)^2 \right. & \nonumber \\
  &= \left( 1 - \frac{2}{\pi} u \right)^3 \cdot v^3 \cdot \frac{\pi}{2} \cdot \left( \frac{\pi}{4-\pi} \right)^2 & \nonumber \\
  &= \left[ v \left( 1 - \frac{2}{\pi} u \right) \right]^3 \left( \frac{\pi^3}{2(4-\pi)^2} \right) & \nonumber \\
  &= \left( v - \frac{2}{\pi} \right)^3 \left( \frac{\pi^3}{2(4-\pi)^2} \right) &= g(v). \label{eq:lhs-solving-for-lambda}
\end{alignat}

We can see that $g(v)$ is increasing in $v = \frac{1+\lambda^2}{\lambda^2} \geq
1$. Therefore:

\begin{equation}
  \min_{v} g(v) = g(v)|_{v=1} = \left( 1 - \frac{2}{\pi} \right)^3 \left( \frac{\pi^3}{2(4-\pi)^2} \right) = 1.009524 \approx 1,
\end{equation}

which means that the right hand side of \eqref{eq:solving-for-lambda}, which is
supposed to be equal to the left hand side of \eqref{eq:solving-for-lambda},
can't ever be less than 1. Unfortunately, it sometimes is; in particular,
$\frac{np(1-p)}{(1-2p)^2} \to 0$ when $p \to 0$ or $p \to 1$. So if we want a
solution, we must restrict $n$ and $p$ such that

\begin{align}
  \textnormal{\{right hand side of \eqref{eq:solving-for-lambda}\}} &\geq \textnormal{\{min of left hand side of \eqref{eq:solving-for-lambda}\}} \nonumber \\
  \frac{np(1-p)}{(1-2p)^2} &\geq 1 \nonumber \\
  np(1-p) &\geq (1-2p)^2. \label{eq:solving-the-restriction}
\end{align}

Here, two scenarios arise. The first is when we have a fixed $p$ and wish to
find the minimum $n$ necessary to derive a skew-normal approximation. From
\eqref{eq:solving-the-restriction}, solving for $n$ is very simple:

\begin{equation}
  n \geq \frac{(1-2p)^2}{p(1-p)} \;. \label{eq: n for a given p}
\end{equation}

Figure \ref{fig:sn-restriction-least-n} in appendix \ref{app:figures} shows the
least sample size required to estimate $\lambda$ as a function of
$p$.\footnotemark As we would expect, it is larger when $p$ is small and
rapidly goes to 0 as $p$ approaches 0.5; for example, when $p = 0.01$, $n$ must
be $\geq 98$, but at $p = 0.2$, $n$ need only be $\geq 3$, a trivial
requirement to meet.

\footnotetext{Since $Bin(n, p)$ and $Bin(n, 1-p)$ are mirror image curves, it
is often only necessary to examine either $p \in (0, 0.5)$ or $p \in (0.5, 1)$.
We will usually consider the former range of $p$'s. \label{fnote:half-p-range}}

The second scenario, primarily of academic interest, is when $n$ is fixed and
we wish to solve for $p$. In this case, we return to
\eqref{eq:solving-the-restriction} for further factoring:

\begin{align}
  np(1-p) &\geq (1-2p)^2 \nonumber \\
  np - np^2 &\geq 1 - 4p + 4p^2 \nonumber \\
  1 - 4p + 4p^2 - np + np^2 &\leq 0 \nonumber \\
  (n+4)p^2 - (n+4)p + 1 &\leq 0 \label{eq: solving for p}.
\end{align}

We then apply the quadratic formula with $a = n+4$, $b = -(n+4)$, and $c = 1$:

\begin{align*}
  p &= \frac{(n+4) \pm \sqrt{(n+4)^2 - 4 \cdot (n+4) \cdot 1}}{2(n+4)} \\
  &= \frac{(n+4) \pm \sqrt{n^2 + 8n + 16 - 4n - 16}}{2(n+4)} \\
  &= \frac{(n+4) \pm \sqrt{n^2 + 4n}}{2(n+4)} \\
  &= \frac{n+4}{2(n+4)} \pm \frac12 \sqrt{\frac{n(n+4)}{(n+4)^2}} \\
  &= \frac12 \pm \frac12 \sqrt{\frac{n}{n+4}} \;.
\end{align*}

Let $r_1 = \frac12 - \frac12 \sqrt{\frac{n}{n+4}}$ and $r_2 = \frac12 + \frac12
\sqrt{\frac{n}{n+4}}$. (Note that $r_1 < r_2$.) Now we can rewrite \eqref{eq:
solving for p} as

\begin{equation*}
  (p - r_1)(p - r_2) \leq 0.
\end{equation*}

Examining the left hand side, when $p < r_1$, both terms are negative and so
their product is positive; when $p > r_2$, both terms are positive, again
leading the product to be positive. Therefore, our solution lies where $r_1
\leq p \leq r_2$, or more explicitly,

\begin{equation}
 \frac12 - \frac12 \sqrt{\frac{n}{n+4}} \; \leq \; p \; \leq \; \frac12 + \frac12 \sqrt{\frac{n}{n+4}} \;.
\end{equation}

Figure \ref{fig:sn-restriction-p-range} in appendix \ref{app:figures} shows
this range as a function of $n$, this interval grows quickly as $n$ increases,
and for sufficiently large $n$, it becomes almost $(0, 1)$. For example, when
$n=100$, our interval is $(0.00971, 0.99029)$; when $n=500$, it is (0.00199,
0.99801).

Although the presence of these restrictions are somewhat disappointing, we can
console ourselves with the observation that at the same $n$ and $p$, the
skew-normal yields substantially more accurate approximations than the normal
(see section \ref{subsec:mabs}). Thus while imperfect, it is nevertheless an
improvement.
