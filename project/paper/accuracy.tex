\section{Demonstrating Improved Accuracy}
\label{sec:accuracy}

Now comes the time to justify our efforts by comparing the accuracy of our
skew-normal approximation to that of the normal.

We are naturally most interested in cases where other solutions perform poorly.
Recall that when $n$ is small, it is feasible to calculate the binomial
directly, and when $n$ is large or $p$ is close to 0.5, the normal distribution
provides an adequate approximation. Therefore, our interest is primarily in
cases where $n$ is moderate and $p$ is extreme (close to 0 or 1).\footnotemark

\footnotetext{Again, we examine only $p \in (0, 0.5)$. (See footnote
\ref{fnote:half-p-range}.)}

\subsection{Visual Comparison}

The first and most obvious way of judging accuracy is by visual inspection.
Figures \ref{fig:comparison-n25}, \ref{fig:comparison-n50}, and
\ref{fig:comparison-n100} in appendix \ref{app:figures} compare the binomial,
normal, and skew-normal at small values of $p$ for $n=25$, $n=50$, and $n=100$,
respectively.

The graphs show that at moderate $n$ and small $p$, our skew-normal curve
follows the shape of the binomial much more closely than the normal. As $n$
grows, however, the Central Limit Theorem begins to exert its effect and the
normal distribution "catches up" in accuracy. Thus as we would expect, the
skew-normal approximation is of greatest value in the former set of cases.

\subsection{Maximal Absolute Error}
\label{subsec:mabs}

Another more quantitative method of judging accuracy is comparing the maximal
absolute errors of our two approximations, defined by \citet{mabs} as

\begin{equation}
  \textnormal{MABS}(n, p) \eq \max_{k \in \{0, 1,...,n\}} \left| F_{B(n,p)} (k) -  F_{\textnormal{appr}(n,p)}(k + 0.5) \right|
\end{equation}

where $F_{B(n,p)}$ is the cdf of the binomial and $F_{\textnormal{appr}(n,p)}$
is the cdf of either the normal or skew-normal approximation; the 0.5 is a
continuity correction.

Figure \ref{fig:mabs-fixed-n} and \ref{fig:mabs-fixed-p} in appendix
\ref{app:figures} shows the MABS of the skew-normal and normal approximations
as a function of $p$ and $n$, respectively.

In figure \ref{fig:mabs-fixed-n}, the MABS of the normal approximation is more
than four times that of the skew-normal when $p$ is very close to 0. The two
error curves converge and eventually meet as $p$ approaches 0.5 and the
binomial becomes symmetrical.

Similarly, in figure \ref{fig:mabs-fixed-p}, the normal MABS is roughly six
times the skew-normal MABS. Interesting, the error curves converge much more
slowly this time, leading to the pleasant (if surprising) conclusion that when
$p$ is extreme, the skew-normal gains us a measure of accuracy even at very
large $n$.
